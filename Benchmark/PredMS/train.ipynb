{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mordred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sklearn\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import RDLogger\n",
    "import utils\n",
    "from rdkit import Chem\n",
    "from mordred import Calculator, descriptors\n",
    "import warnings , os\n",
    "import subprocess\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_descriptor(smiles_list):\n",
    "    calc = Calculator(descriptors, ignore_3D = True)\n",
    "    mols = []\n",
    "    for smi in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smi) \n",
    "        if mol != None:\n",
    "            mols.append(mol)\n",
    "            \n",
    "    for each_mol in mols:\n",
    "        try:\n",
    "            AllChem.EmbedMolecule(each_mol, randomSeed=42)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    df = calc.pandas(mols, nproc=1)\n",
    "    new_df = df.select_dtypes(include=['float64', 'int'])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../MS_BACL/data_OURS/train.csv', index_col=None)\n",
    "test  = pd.read_csv('../MS_BACL/data_OURS/test.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = calc_descriptor(train['SMILES'])\n",
    "test_data = calc_descriptor(test['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[train_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.values\n",
    "y_train = train['Label'].values\n",
    "\n",
    "X_test = test_data.values\n",
    "y_test = test['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create Random Forest model with the exact parameters from the paper\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,      # number of trees = 500\n",
    "    max_depth=30,          # maximum depth of trees = 30\n",
    "    min_samples_leaf=3,    # minimum samples in leaf = 3\n",
    "    min_samples_split=2,   # minimum samples for split = 2\n",
    "    random_state=42        # for reproducibility\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = rf_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredMS_probs = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (probs>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test['Label'], probs)\n",
    "acc = accuracy_score(test['Label'], preds)\n",
    "prc = precision_score(test['Label'], preds)\n",
    "rec = recall_score(test['Label'], preds)\n",
    "f1s = f1_score(test['Label'], preds)\n",
    "\n",
    "# f1s = (2 * prc * rec) / (rec + prc)\n",
    "mcc = matthews_corrcoef(test['Label'], preds)\n",
    "\n",
    "print(f'auc : {auc:.4f}')\n",
    "print(f'acc : {acc:.4f}')\n",
    "print(f'prc : {prc:.4f}')\n",
    "print(f'rec : {rec:.4f}')\n",
    "print(f'f1s : {f1s:.4f}')\n",
    "print(f'mcc : {mcc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"Calculate all metrics at once\"\"\"\n",
    "    return {\n",
    "        'auc': roc_auc_score(y_true, y_prob),\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'prc': precision_score(y_true, y_pred),\n",
    "        'rec': recall_score(y_true, y_pred),\n",
    "        'f1s': f1_score(y_true, y_pred),\n",
    "        'mcc': matthews_corrcoef(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def bootstrap_metrics(test_labels, predicted_probs, predicted_labels, n_iterations=1000, ci=95):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence intervals for all metrics\n",
    "    \"\"\"\n",
    "    # Initialize storage for bootstrap estimates\n",
    "    bootstrap_estimates = {metric: [] for metric in ['auc', 'acc', 'prc', 'rec', 'f1s', 'mcc']}\n",
    "    \n",
    "    # Get sample size\n",
    "    n_samples = len(test_labels)\n",
    "    \n",
    "    # Calculate actual metrics once\n",
    "    actual_metrics = calculate_metrics(test_labels, predicted_labels, predicted_probs)\n",
    "    \n",
    "    # Bootstrap iterations\n",
    "    for _ in range(n_iterations):\n",
    "        # Generate bootstrap sample indices\n",
    "        indices = np.random.randint(0, n_samples, n_samples)\n",
    "        \n",
    "        # Calculate metrics for this bootstrap sample\n",
    "        bootstrap_metrics = calculate_metrics(\n",
    "            test_labels[indices],\n",
    "            predicted_labels[indices],\n",
    "            predicted_probs[indices]\n",
    "        )\n",
    "        \n",
    "        # Store this bootstrap sample's metrics\n",
    "        for metric, value in bootstrap_metrics.items():\n",
    "            bootstrap_estimates[metric].append(value)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    alpha = (100 - ci) / 100\n",
    "    results = {}\n",
    "    \n",
    "    for metric in bootstrap_estimates.keys():\n",
    "        # Get confidence intervals\n",
    "        ci_lower = np.percentile(bootstrap_estimates[metric], alpha * 50)\n",
    "        ci_upper = np.percentile(bootstrap_estimates[metric], 100 - alpha * 50)\n",
    "        \n",
    "        results[metric] = {\n",
    "            'value': actual_metrics[metric],\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 실행 예시\n",
    "# test['Label']와 probs, preds는 numpy array로 변환되어 있다고 가정\n",
    "test_labels = np.array(test['Label'])\n",
    "predicted_probs = np.array(probs)\n",
    "predicted_labels = np.array(preds)\n",
    "\n",
    "# Calculate bootstrap results\n",
    "bootstrap_results = bootstrap_metrics(test_labels, predicted_probs, predicted_labels)\n",
    "\n",
    "# Print results with confidence intervals\n",
    "for metric, result in bootstrap_results.items():\n",
    "    print(f\"{metric:3s}: {result['value']:.4f}({result['ci_lower']:.4f}-{result['ci_upper']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
